#
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
#

BeamEnginesDataflow.OptionsAppName.Label=Application name
BeamEnginesDataflow.OptionsAppName.ToolTip=The name of the Dataflow job being executed as it appears in Dataflow's jobs list and job details
BeamEnginesDataflow.OptionsAutoScalingAlgorithm.Label=Auto scaling algorithm
BeamEnginesDataflow.OptionsAutoScalingAlgorithm.ToolTip=The autoscaling mode for your Dataflow job.\n Possible values are THROUGHPUT_BASED to enable autoscaling, or NONE to disable.\n See https://cloud.google.com/dataflow/service/dataflow-service-desc#Autotuning \n to learn more about how autoscaling works in the Dataflow managed service.
BeamEnginesDataflow.OptionsDiskSize.Label=Disk size in GB
BeamEnginesDataflow.OptionsDiskSize.ToolTip=The disk size, in gigabytes, to use on each remote Compute Engine worker instance.\n If set, specify at least 30 GB to account for the worker boot image and local logs
BeamEnginesDataflow.OptionsMaxNumberOfWorkers.Label=Maximum number of workers
BeamEnginesDataflow.OptionsMaxNumberOfWorkers.ToolTip=The maximum number of workers to use for the workerpool. This options limits the size of the workerpool for the lifetime of the job, including pipeline updates. If left unspecified, the Dataflow service will compute a ceiling.
BeamEnginesDataflow.OptionsNetwork.Label=Network
BeamEnginesDataflow.OptionsNetwork.ToolTip=GCE network for launching workers. For more information, see the reference documentation https://cloud.google.com/compute/docs/networking. Default is up to the Dataflow service.
BeamEnginesDataflow.OptionsNumberOfWorkers.Label=Initial number of workers
BeamEnginesDataflow.OptionsNumberOfWorkers.ToolTip=The initial number of Google Compute Engine instances to use when executing your pipeline. \n This option determines how many workers the Dataflow service starts up when your job begins.
BeamEnginesDataflow.OptionsProjectID.Label=Project ID
BeamEnginesDataflow.OptionsProjectID.ToolTip=The project ID for your Google Cloud Project
BeamEnginesDataflow.OptionsPublicIP.Label=Use public IPs?
BeamEnginesDataflow.OptionsPublicIP.ToolTip=Specifies whether worker pools should be started with public IP addresses. WARNING: This feature is experimental. You must be allowlisted to use it.
BeamEnginesDataflow.OptionsRegion.Label=Region
BeamEnginesDataflow.OptionsRegion.ToolTip=Specifies a Compute Engine region for launching worker instances to run your pipeline.\n This option is used to run workers in a different location than the region used to deploy, manage, and monitor jobs.\n The zone for workerRegion is https://cloud.google.com/dataflow/docs/concepts/regional-endpoints#autozone.\n \n Note: This option cannot be combined with workerZone or zone.\n \n More information: https://cloud.google.com/dataflow/docs/concepts/regional-endpoints
BeamEnginesDataflow.OptionsServiceAccount.Label=Service account
BeamEnginesDataflow.OptionsServiceAccount.ToolTip=Run the job as a specific service account, instead of the default GCE robot
BeamEnginesDataflow.OptionsServiceOptions.Label=Dataflow service options
BeamEnginesDataflow.OptionsServiceOptions.ToolTip=Comma separated list of service options.  Service options are set by the user and configure the service. This decouples service side feature availability from the Apache Beam release cycle.
BeamEnginesDataflow.OptionsStagingLocation.Label=Staging location
BeamEnginesDataflow.OptionsStagingLocation.ToolTip=Cloud Storage path for staging local files. \n Must be a valid Cloud Storage URL, beginning with gs://
BeamEnginesDataflow.OptionsSubNetwork.Label=Subnetwork
BeamEnginesDataflow.OptionsSubNetwork.ToolTip=GCE subnetwork for launching workers. For more information, see the reference documentation https://cloud.google.com/compute/docs/networking. Default is up to the Dataflow service.
BeamEnginesDataflow.OptionsWorkerDiskType.Label=Worker disk type
BeamEnginesDataflow.OptionsWorkerDiskType.ToolTip=The type of persistent disk to use, specified by a full URL of the disk type resource.\n For more information see:\n https://cloud.google.com/compute/docs/disks#pdspecs
BeamEnginesDataflow.OptionsWorkerMachineType.Label=Worker machine type
BeamEnginesDataflow.OptionsWorkerMachineType.ToolTip=The Compute Engine machine type that Dataflow uses when starting worker VMs.\n You can use any of the available Compute Engine machine type families as well as custom machine types.\n \n For best results, use n1 machine types.\n Shared core machine types, such as f1 and g1 series workers, are not supported under the Dataflow Service Level Agreement.\n \n Note that Dataflow bills by the number of vCPUs and GB of memory in workers.\n Billing is independent of the machine type family.\n Check https://cloud.google.com/compute/docs/machine-types to see the machine types for reference.
BeamEnginesDataflow.OptionsZone.Label=Zone
BeamEnginesDataflow.OptionsZone.ToolTip=Specifies a Compute Engine zone for launching worker instances to run your pipeline.\n This option is used to run workers in a different location than the region used to deploy, manage, and monitor jobs.\n \n Note: This option cannot be combined with workerRegion or zone.
