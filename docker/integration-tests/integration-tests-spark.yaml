# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

services:
  integration_test_spark:
    extends:
      file: integration-tests-base.yaml
      service: integration_test
    depends_on:
      - spark
      - spark-worker
      - namenode
      - datanode
      - datanode2
      - datanode3
      - resourcemanager
      - nodemanager
      - hiveserver
      - hive-metastore
    links:
      - spark
      - spark-worker
      - namenode
      - datanode
      - datanode2
      - datanode3
      - resourcemanager
      - nodemanager
      - hiveserver
      - hive-metastore
  spark:
    build:
      context: ../../docker/integration-tests/spark/.
      dockerfile: Dockerfile.master
    environment:
      - INIT_DAEMON_STEP=setup_spark
    ports:
      - 8080
      - 7077
  spark-worker:
    build:
      context: ../../docker/integration-tests/spark/.
      dockerfile: Dockerfile.worker
    depends_on:
      - spark
    environment:
      - "SPARK_MASTER=spark://spark:7077"
    ports:
      - 8081
  namenode:
    build: 
      dockerfile: Dockerfile.hadoop
      context: ../../docker/integration-tests/spark/.
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870
      - 8020
    env_file:
      - ./spark/config
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
  datanode:
    build: 
      dockerfile: Dockerfile.hadoop
      context: ../../docker/integration-tests/spark/.
    command: ["hdfs", "datanode"]
    env_file:
      - ./spark/config
  datanode2:
    build: 
      dockerfile: Dockerfile.hadoop
      context: ../../docker/integration-tests/spark/.
    command: ["hdfs", "datanode"]
    env_file:
      - ./spark/config
  datanode3:
    build: 
      dockerfile: Dockerfile.hadoop
      context: ../../docker/integration-tests/spark/.
    command: ["hdfs", "datanode"]
    env_file:
      - ./spark/config
  resourcemanager:
    build: 
      dockerfile: Dockerfile.hadoop
      context: ../../docker/integration-tests/spark/.
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
       - 8088
    env_file:
      - ./spark/config
    volumes:
      - ./test.sh:/opt/test.sh
  nodemanager:
    build: 
      dockerfile: Dockerfile.hadoop
      context: ../../docker/integration-tests/spark/.
    command: ["yarn", "nodemanager"]
    env_file:
      - ./spark/config
  hiveserver:
    image: apache/hive:4.0.0-beta-1
    ports:
      - 10000
      - 10002
    depends_on:
      - hive-metastore
    environment:
      - SERVICE_NAME=hiveserver2
      - SERVICE_OPTS=-Dhive.metastore.uris=thrift://hive-metastore:9083
    volumes:
      - ./spark/hive-site.xml:/opt/hive/conf/hive-site.xml
  hive-metastore:
    image: apache/hive:4.0.0-beta-1
    ports:
      - 9083
    environment:
      - SERVICE_NAME=metastore
