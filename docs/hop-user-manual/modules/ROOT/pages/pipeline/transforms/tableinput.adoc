////
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at
  http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
////
:documentationPath: /pipeline/transforms/
:language: en_US
:description: The Table Input transform is used to read information from a database, using a connection and SQL.
:openvar: ${
:closevar: }

= image:transforms/icons/tableinput.svg[Table Input transform Icon, role="image-doc-icon"] Table Input

[%noheader,cols="3a,1a", role="table-no-borders"]
|===
a|
== Description

The Table Input transform is one of the most frequently used transforms in Apache Hop. It reads data from a relational database using a connection and an SQL query, and introduces that data into a pipeline.

This transform supports parameterization using:
- Apache Hop variables (e.g., `${customerId}`)
- Input fields from a previous transform (JDBC prepared statements using `?` placeholders)

These features make Table Input ideal for dynamic and reusable data retrieval logic.

|
== Supported Engines
[%noheader,cols="2,1a",frame=none, role="table-supported-engines"]
!===
!Hop Engine! image:check_mark.svg[Supported, 24]
!Spark! image:check_mark.svg[Supported, 24]
!Flink! image:check_mark.svg[Supported, 24]
!Dataflow! image:check_mark.svg[Supported, 24]
!===
|===

== Getting Started: Generate a Basic SQL Query

You can auto-generate a query using the `Get SQL select statement` button.

This opens the database explorer, allowing you to select a table or view. Once selected, you can choose to generate:

- A full column list: `SELECT col_a, col_b, col_c FROM my_table;`
- A wildcard query: `SELECT * FROM my_table;`

== Use Fields from a Previous Transform

To pass dynamic values into your SQL query at runtime, use the `Insert data from transform` option. This creates a JDBC Prepared Statement using `?` placeholders.

Use `?` in your SQL where values from the input transform should be inserted. Values are passed in the order of fields in the incoming stream. Use a `Select Values` transform to ensure the correct field order.

Prepared statements:

- Improve security by preventing SQL injection
- Cannot parameterize all parts of a SQL statement (e.g., `IN (?)` or table names)

You can also combine this with variable substitution.

=== Examples

.Parameterized query using input fields:

image:transforms/tableinput/table-input-parametrized-query.png[Parameterized query using input fields, width="45%"]

[source,sql]
----
SELECT *
FROM public.tableinput
WHERE id > ?;
----

- Replace variables in script: *unchecked*
- Insert data from transform: *Select the previous transform providing NameId and AddressId*

Sample pipeline:
link:https://github.com/apache/hop/blob/main/plugins/transforms/tableinput/src/main/samples/transforms/tableinput-accept-input.hpl[tableinput-accept-input.hpl]

.Using a date range from a system transform:

image:transforms/tableinput/table-input-date-range.png[Using a date range from a system transform, width="45%"]

[source,sql]
----
SELECT *
FROM public.tableinput
WHERE changed_date BETWEEN ? AND ?;
----

- Use a `Get System Info` transform to generate the start and end dates
- Insert those dates using `Insert data from transform`

== Use Variables in Your SQL Query

image:transforms/tableinput/table-input-variables-in-sql.png[Use Variables in Your SQL Query, width="45%"]

If your query includes Hop variables, enable `Replace variables in script`. This performs a simple string replacement *before* the query is sent to the database.

[source,sql]
----
SELECT *
FROM public.tableinput
WHERE id > ${PRM_ID};
----

- `${PRM_ID}` is defined as a pipeline variable (e.g., via parameters or `Set Variables` transform)

image:transforms/tableinput/table-input-variables-in-sql-parameter-definition.png[Parameter definition in pipeline, width="45%"]

- This gives you full control over the query structure
- Combine with `?` placeholders if needed

NOTE: Variable substitution happens before execution and does not protect against SQL injection.

- Replace variables in script: *checked*
- Insert data from transform: *leave empty*

Sample pipeline:
link:https://github.com/apache/hop/blob/main/plugins/transforms/tableinput/src/main/samples/transforms/tableinput-variables.hpl[tableinput-variables.hpl]

== Using Both Variables and Prepared Statements

You can combine both techniques in a single query:

image:transforms/tableinput/table-input-combine-variables-and-prepared-statements.png[Using Both Variables and Prepared Statements, width="45%"]

[source,sql]
----
SELECT *
FROM public.tableinput
where id > ${PRM_ID} AND lastdate > ?;
----

- `${startDate}` is a pipeline variable
- `?` is a parameter provided by the input stream

== Best Practices and Pro Tips

== Pro Tips

TIP: 	The Table input transform does not pass input data to the output, only fields inside the query are returned to the pipeline so all other variables and data will be lost. You can solve this by adding the variable as a field in the query or put a Get variables transform behind the table input.

TIP: If you are getting unexpected query results, try clearing the database cache. Click the *broom icon* or go to *Tools > Clear DB Cache*. After clearing, click OK, save your pipeline, and reopen it if needed.

TIP: A cartesian join transform will combine a different number of fields from multiple table inputs without requiring key join fields.

TIP: Using the "insert data from transform" drop down will block until the transform selected has completed.

TIP: For better performance with large datasets you can use indexed columns in `WHERE` clauses and avoid `SELECT *` and only retrieve needed fields.

== Dynamic SQL with Metadata Injection

Table Input can be used in a metadata-driven pipeline. For example, create a template pipeline with a generic query:

[source,sql]
----
SELECT *
FROM ${tableName}
WHERE ${condition}
----

Then use a Metadata Injection transform to inject actual values (e.g., from a CSV or database).

.Steps:
1. Create a template pipeline with Table Input
2. Use `${tableName}` and `${condition}` as placeholders
3. In a separate pipeline, use Metadata Injection to inject values into the SQL field
4. Execute the injected pipeline

This allows you to create **reusable and dynamic** pipelines without editing SQL manually.

You can inject metadata into the following fields of the Table Input transform:

- Connection
- SQL
- Replace variables in script?
- Insert data from transform
- Execute for each row?
- Limit size

== Options

[options="header"]
|===
|Option|Description

|Transform name|Name of the transform instance.
|Connection|Database connection to execute the query against.
|SQL|SQL statement used to retrieve data. Use `Get SQL select statement` to auto-generate.
|Replace variables in script?|Enable to substitute variables (e.g., `${param}`) in your SQL before execution.
|Insert data from transform|Select a transform to use its fields as input for `?` parameters in a prepared statement.
|Execute for each row?|Runs the SQL query once for each incoming row, using that row’s values as parameters. Only applies when “Insert data from transform” is enabled. Useful for row-specific lookups, but may be slower on large datasets.
|Limit size|Number of rows to return. `0` means no limit.
|===

